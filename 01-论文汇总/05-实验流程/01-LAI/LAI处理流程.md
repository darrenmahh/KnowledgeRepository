```python
#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
LAIæ•°æ®æ’è¡¥å’Œåˆå¹¶è„šæœ¬
å¤„ç†LAIæ—¶é—´åºåˆ—æ•°æ®å¹¶ä¸é€šé‡æ•°æ®åˆå¹¶
"""

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import StandardScaler
import warnings
warnings.filterwarnings('ignore')

def load_and_prepare_lai_data(lai_file):
    """åŠ è½½å’Œé¢„å¤„ç†LAIæ•°æ®"""
    print("æ­£åœ¨åŠ è½½LAIæ•°æ®...")
    lai_df = pd.read_csv(lai_file)
    
    # è½¬æ¢æ—¶é—´æ ¼å¼
    lai_df['record_time'] = pd.to_datetime(lai_df['record_time'])
    
    # æ·»åŠ æ—¶é—´ç‰¹å¾ç”¨äºæ’è¡¥
    lai_df['year'] = lai_df['record_time'].dt.year
    lai_df['month'] = lai_df['record_time'].dt.month
    lai_df['day_of_year'] = lai_df['record_time'].dt.dayofyear
    lai_df['season'] = lai_df['month'].map({
        12: 'winter', 1: 'winter', 2: 'winter',
        3: 'spring', 4: 'spring', 5: 'spring',
        6: 'summer', 7: 'summer', 8: 'summer',
        9: 'autumn', 10: 'autumn', 11: 'autumn'
    })
    
    print(f"åŸå§‹LAIæ•°æ®å½¢çŠ¶: {lai_df.shape}")
    print(f"ç«™ç‚¹æ•°é‡: {lai_df['site_id'].nunique()}")
    print(f"æ—¶é—´èŒƒå›´: {lai_df['record_time'].min()} åˆ° {lai_df['record_time'].max()}")
    
    return lai_df

def interpolate_lai_for_site(site_data):
    """ä¸ºå•ä¸ªç«™ç‚¹æ’è¡¥LAIæ•°æ®"""
    site_id = site_data['site_id'].iloc[0]
    print(f"æ­£åœ¨ä¸ºç«™ç‚¹ {site_id} æ’è¡¥LAIæ•°æ®...")
    
    # åˆ›å»ºå®Œæ•´çš„æ—¶é—´åºåˆ—
    start_date = site_data['record_time'].min()
    end_date = site_data['record_time'].max()
    
    # åˆ›å»ºæ¯æ—¥æ—¶é—´åºåˆ—
    date_range = pd.date_range(start=start_date, end=end_date, freq='D')
    complete_df = pd.DataFrame({
        'record_time': date_range,
        'site_id': site_id
    })
    
    # åˆå¹¶ç°æœ‰æ•°æ®
    complete_df = complete_df.merge(site_data, on=['record_time', 'site_id'], how='left')
    
    # æ·»åŠ æ—¶é—´ç‰¹å¾
    complete_df['year'] = complete_df['record_time'].dt.year
    complete_df['month'] = complete_df['record_time'].dt.month
    complete_df['day_of_year'] = complete_df['record_time'].dt.dayofyear
    complete_df['season'] = complete_df['month'].map({
        12: 'winter', 1: 'winter', 2: 'winter',
        3: 'spring', 4: 'spring', 5: 'spring',
        6: 'summer', 7: 'summer', 8: 'summer',
        9: 'autumn', 10: 'autumn', 11: 'autumn'
    })
    
    # å­£èŠ‚æ€§æ’è¡¥
    season_medians = site_data.groupby('season')['LAI'].median()
    for season in season_medians.index:
        mask = (complete_df['season'] == season) & complete_df['LAI'].isna()
        complete_df.loc[mask, 'LAI'] = season_medians[season]
    
    # çº¿æ€§æ’å€¼
    complete_df['LAI'] = complete_df['LAI'].interpolate(method='linear')
    
    # å‘å‰å¡«å……å’Œå‘åå¡«å……å¤„ç†è¾¹ç•Œå€¼
    complete_df['LAI'] = complete_df['LAI'].fillna(method='ffill').fillna(method='bfill')
    
    # å¦‚æœè¿˜æœ‰ç¼ºå¤±å€¼ï¼Œç”¨å…¨ç«™ç‚¹å‡å€¼å¡«å……
    if complete_df['LAI'].isna().any():
        overall_mean = site_data['LAI'].mean()
        complete_df['LAI'] = complete_df['LAI'].fillna(overall_mean)
    
    return complete_df[['site_id', 'record_time', 'LAI']]

def interpolate_lai_data(lai_df):
    """æ’è¡¥æ‰€æœ‰ç«™ç‚¹çš„LAIæ•°æ®"""
    print("å¼€å§‹LAIæ•°æ®æ’è¡¥...")
    
    interpolated_dfs = []
    for site_id in lai_df['site_id'].unique():
        site_data = lai_df[lai_df['site_id'] == site_id].copy()
        interpolated_site = interpolate_lai_for_site(site_data)
        interpolated_dfs.append(interpolated_site)
    
    # åˆå¹¶æ‰€æœ‰ç«™ç‚¹çš„æ’è¡¥æ•°æ®
    interpolated_lai = pd.concat(interpolated_dfs, ignore_index=True)
    
    print(f"æ’è¡¥åLAIæ•°æ®å½¢çŠ¶: {interpolated_lai.shape}")
    return interpolated_lai

def merge_with_flux_data(interpolated_lai, flux_file, output_file):
    """å°†æ’è¡¥åçš„LAIæ•°æ®ä¸é€šé‡æ•°æ®åˆå¹¶"""
    print("æ­£åœ¨åŠ è½½é€šé‡æ•°æ®...")
    flux_df = pd.read_csv(flux_file)
    flux_df['record_time'] = pd.to_datetime(flux_df['record_time'])
    
    print(f"é€šé‡æ•°æ®å½¢çŠ¶: {flux_df.shape}")
    print(f"é€šé‡æ•°æ®ç«™ç‚¹: {sorted(flux_df['site_id'].unique())}")
    
    # åˆå¹¶æ•°æ®
    print("æ­£åœ¨åˆå¹¶LAIæ•°æ®å’Œé€šé‡æ•°æ®...")
    merged_df = flux_df.merge(
        interpolated_lai[['site_id', 'record_time', 'LAI']], 
        on=['site_id', 'record_time'], 
        how='left'
    )
    
    print(f"åˆå¹¶åæ•°æ®å½¢çŠ¶: {merged_df.shape}")
    print(f"LAIç¼ºå¤±å€¼æ•°é‡: {merged_df['LAI'].isna().sum()}")
    
    # å¯¹äºæ²¡æœ‰LAIæ•°æ®çš„ç«™ç‚¹ï¼Œä½¿ç”¨åŒç±»å‹ç«™ç‚¹çš„å‡å€¼
    if merged_df['LAI'].isna().any():
        print("ä¸ºç¼ºå¤±LAIçš„ç«™ç‚¹å¡«å……å€¼...")
        
        # å¦‚æœæœ‰land_use_typeåˆ—ï¼ŒæŒ‰åœŸåœ°åˆ©ç”¨ç±»å‹å¡«å……
        if 'land_use_type' in merged_df.columns:
            for land_use in merged_df['land_use_type'].unique():
                if pd.isna(land_use):
                    continue
                mask = (merged_df['land_use_type'] == land_use) & merged_df['LAI'].isna()
                if mask.any():
                    # æ‰¾åˆ°æœ‰LAIæ•°æ®çš„åŒç±»å‹ç«™ç‚¹çš„å‡å€¼
                    same_type_lai = merged_df[
                        (merged_df['land_use_type'] == land_use) & 
                        merged_df['LAI'].notna()
                    ]['LAI'].mean()
                    
                    if not pd.isna(same_type_lai):
                        merged_df.loc[mask, 'LAI'] = same_type_lai
        
        # å¦‚æœè¿˜æœ‰ç¼ºå¤±å€¼ï¼Œç”¨å…¨éƒ¨ç«™ç‚¹çš„å‡å€¼å¡«å……
        if merged_df['LAI'].isna().any():
            overall_mean = interpolated_lai['LAI'].mean()
            merged_df['LAI'] = merged_df['LAI'].fillna(overall_mean)
    
    # ä¿å­˜åˆå¹¶åçš„æ•°æ®
    merged_df.to_csv(output_file, index=False)
    print(f"åˆå¹¶åçš„æ•°æ®å·²ä¿å­˜åˆ°: {output_file}")
    
    # ç»Ÿè®¡ä¿¡æ¯
    print("\n=== LAIç‰¹å¾ç»Ÿè®¡ ===")
    print(f"è®°å½•æ•°: {len(merged_df):,}")
    print(f"LAIå‡å€¼: {merged_df['LAI'].mean():.3f}")
    print(f"LAIæ ‡å‡†å·®: {merged_df['LAI'].std():.3f}")
    print(f"LAIèŒƒå›´: [{merged_df['LAI'].min():.3f}, {merged_df['LAI'].max():.3f}]")
    print(f"LAIç¼ºå¤±å€¼: {merged_df['LAI'].isna().sum()}")
    
    # æŒ‰ç«™ç‚¹ç»Ÿè®¡
    print("\n=== å„ç«™ç‚¹LAIç»Ÿè®¡ ===")
    lai_stats = merged_df.groupby('site_id')['LAI'].agg(['count', 'mean', 'std', 'min', 'max'])
    print(lai_stats.round(3))
    
    return merged_df

def update_summary_file(summary_file, merged_df):
    """æ›´æ–°FINAL_DATA_SUMMARY.txtæ–‡ä»¶"""
    print("æ­£åœ¨æ›´æ–°æ•°æ®æ±‡æ€»æ–‡ä»¶...")
    
    # è®¡ç®—LAIä¸GPPçš„ç›¸å…³æ€§
    if 'gpp' in merged_df.columns:
        lai_gpp_corr = merged_df['LAI'].corr(merged_df['gpp'])
    else:
        lai_gpp_corr = None
    
    new_content = f"""

### 17. å¶é¢ç§¯æŒ‡æ•°LAIç‰¹å¾ (æ–‡ä»¶17) - æœ€æ–°æ•°æ®é›†
**ç”Ÿæˆæ–‡ä»¶**: 17_add_LAI.csv
**åŸºäºæ–‡ä»¶**: 16_add_ta_swc.csv
**æ–°å¢ç‰¹å¾**: LAI (å¶é¢ç§¯æŒ‡æ•°)
**ç”Ÿæ€å­¦æ„ä¹‰**: æ¤è¢«å¶é¢ç§¯å¯†åº¦ï¼Œåæ˜ æ¤è¢«å…‰åˆä½œç”¨èƒ½åŠ›
**æ•°æ®æ¥æº**: åŒ—äº¬ç«™ç‚¹LAIæ—¶é—´åºåˆ—æ•°æ® (Google Earth Engine)
**æ’è¡¥æ–¹æ³•**: å¤šå±‚æ¬¡æ’è¡¥ç­–ç•¥
  - å­£èŠ‚æ€§ä¸­ä½æ•°æ’è¡¥
  - çº¿æ€§æ’å€¼
  - å‰å‘åå‘å¡«å……
  - ç«™ç‚¹ç±»å‹å‡å€¼å¡«å……
**ç»Ÿè®¡ä¿¡æ¯**:
  - è®°å½•æ•°: {len(merged_df):,}
  - ç‰¹å¾æ•°: {len(merged_df.columns)} (38 + 1æ–°å¢)
  - ç¼ºå¤±å€¼: {merged_df['LAI'].isna().sum()} ({merged_df['LAI'].isna().sum()/len(merged_df)*100:.1f}%è¦†ç›–ç‡)
  - å‡å€¼: {merged_df['LAI'].mean():.3f}
  - æ ‡å‡†å·®: {merged_df['LAI'].std():.3f}
  - èŒƒå›´: [{merged_df['LAI'].min():.3f}, {merged_df['LAI'].max():.3f}]"""
    
    if lai_gpp_corr is not None:
        new_content += f"""
**ä¸GPPç›¸å…³æ€§**: {lai_gpp_corr:.3f}"""
    
    new_content += f"""
**ååŒæ•ˆåº”**: LAIä½œä¸ºé‡è¦çš„æ¤è¢«ç»“æ„å‚æ•°ï¼Œä¸å…‰è°±æŒ‡æ•°å½¢æˆäº’è¡¥

## ğŸ¯ æœ€ç»ˆæ•°æ®é›†ç‰¹å¾æ¸…å•æ›´æ–° (17_add_LAI.csv)

### æ–°å¢æ¤è¢«ç»“æ„ç‰¹å¾ (1ä¸ªç‰¹å¾)
39. LAI - å¶é¢ç§¯æŒ‡æ•° (æ¤è¢«å¶é¢ç§¯å¯†åº¦)

**å®Œæ•´ç‰¹å¾åˆ†ç±» (39ä¸ªç‰¹å¾)ï¼š**
- ğŸ“ åŸºç¡€ä¿¡æ¯ (2ä¸ª): site_id, record_time
- ğŸŒ¡ï¸ é€šé‡ç‰¹å¾ (10ä¸ª): æ°”è±¡ã€è¾å°„ã€GPPã€APARç­‰
- ğŸ—ºï¸ åœ°ç†ç‰¹å¾ (6ä¸ª): ç»çº¬åº¦ã€æµ·æ‹”ã€åœŸåœ°åˆ©ç”¨
- ğŸ›°ï¸ å…‰è°±ç‰¹å¾ (6ä¸ª): Blue, Green, Red, NIR, SWIR1, SWIR2
- ğŸ“Š æ¤è¢«æŒ‡æ•° (3ä¸ª): NDVI, EVI, NIRv
- â° æ—¶é—´ç‰¹å¾ (1ä¸ª): DOY
- ğŸŒ¿ ç”Ÿé•¿å­£ç‰¹å¾ (3ä¸ª): å¼€å§‹/å³°å€¼/ç»“æŸæ—¥
- â³ æ»åç‰¹å¾ (5ä¸ª): APAR(t-1), VPD(t-1), swc(t-1/3/7)
- ğŸ¤ äº¤äº’ç‰¹å¾ (2ä¸ª): APAR_NDVI_interaction, ta_swc_interaction
- ğŸŒ± æ¤è¢«ç»“æ„ (1ä¸ª): LAI

**LAIç‰¹å¾ä¼˜åŠ¿**:
- ğŸ¯ ç›´æ¥åæ˜ æ¤è¢«å…‰åˆé¢ç§¯
- ğŸŒ¿ è¡¥å……å…‰è°±æŒ‡æ•°ä¿¡æ¯
- ğŸ“Š å¢å¼ºç”Ÿæ€è¿‡ç¨‹ç†è§£
- ğŸ”¬ æä¾›æ¤è¢«ç»“æ„ç»†èŠ‚

è¿™ä½¿å¾—æ•°æ®é›†åœ¨æ¤è¢«ç”Ÿæ€å»ºæ¨¡æ–¹é¢æ›´åŠ å®Œæ•´å’Œç²¾ç¡®ã€‚

"""
    
    # è¯»å–ç°æœ‰æ–‡ä»¶å¹¶æ·»åŠ æ–°å†…å®¹
    try:
        with open(summary_file, 'r', encoding='utf-8') as f:
            existing_content = f.read()
        
        # åœ¨æ–‡ä»¶æœ«å°¾æ·»åŠ æ–°å†…å®¹
        updated_content = existing_content + new_content
        
        with open(summary_file, 'w', encoding='utf-8') as f:
            f.write(updated_content)
        
        print(f"æ•°æ®æ±‡æ€»æ–‡ä»¶å·²æ›´æ–°: {summary_file}")
    except Exception as e:
        print(f"æ›´æ–°æ±‡æ€»æ–‡ä»¶æ—¶å‡ºé”™: {e}")

def main():
    """ä¸»å‡½æ•°"""
    print("=== LAIæ•°æ®æ’è¡¥å’Œåˆå¹¶å¤„ç† ===\n")
    
    # æ–‡ä»¶è·¯å¾„
    lai_file = "gee_data/Beijing_Sites_LAI_TimeSeries_Data.csv"
    flux_file = "processed_qc_flux_data/16_add_ta_swc.csv"
    output_file = "processed_qc_flux_data/17_add_LAI.csv"
    summary_file = "processed_qc_flux_data/FINAL_DATA_SUMMARY.txt"
    
    try:
        # 1. åŠ è½½å’Œé¢„å¤„ç†LAIæ•°æ®
        lai_df = load_and_prepare_lai_data(lai_file)
        
        # 2. æ’è¡¥LAIæ•°æ®
        interpolated_lai = interpolate_lai_data(lai_df)
        
        # 3. ä¸é€šé‡æ•°æ®åˆå¹¶
        merged_df = merge_with_flux_data(interpolated_lai, flux_file, output_file)
        
        # 4. æ›´æ–°æ±‡æ€»æ–‡ä»¶
        update_summary_file(summary_file, merged_df)
        
        print("\n=== å¤„ç†å®Œæˆ ===")
        print(f"âœ… æœ€ç»ˆæ•°æ®é›†: {output_file}")
        print(f"âœ… ç‰¹å¾æ•°é‡: {len(merged_df.columns)}")
        print(f"âœ… è®°å½•æ•°é‡: {len(merged_df):,}")
        print(f"âœ… LAIè¦†ç›–ç‡: {(1-merged_df['LAI'].isna().sum()/len(merged_df))*100:.1f}%")
        
    except Exception as e:
        print(f"âŒ å¤„ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
```